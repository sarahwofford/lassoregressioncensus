{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sarah Wofford\n",
    "#Digital Avertising - MSBuAn - CU Boulder\n",
    "##LASSO Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'B01001007'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------#\n",
    "#Import packages and items into Python to aid#\n",
    "#LASSO regression model                      #\n",
    "#--------------------------------------------#\n",
    "\n",
    "#Data is available on GitHub\n",
    "\n",
    "# Importing items and packages to work help make the regression work with LASSO\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean\n",
    "\n",
    "#--------------------------------------------#\n",
    "#  Import and clean up data before analysis  #\n",
    "#--------------------------------------------#\n",
    "\n",
    "\n",
    "# import data into Python\n",
    "data = 'finalmaster-ratios.csv'\n",
    "\n",
    "#import data into Panda as a data frame\n",
    "alldata = pd.read_csv('finalmaster-ratios.csv')\n",
    "\n",
    "# #Purchases = outcome variable in data set\n",
    "# 189 predictor variables in data set\n",
    "\n",
    "# Get a list of the variables from alldata\n",
    "allvariablenames = list(alldata.columns.values)\n",
    "\n",
    "\n",
    "# need to remove the first 8 variables from the list\n",
    "\n",
    "allvariablenames.pop(0) #remove #Purchases\n",
    "allvariablenames.pop(0) #remove B01001001\n",
    "allvariablenames.pop(0) #remove B01001002\n",
    "allvariablenames.pop(0) #remove B01001003\n",
    "allvariablenames.pop(0) #remove B01001004\n",
    "allvariablenames.pop(0) #remove B01001005\n",
    "allvariablenames.pop(0) #remove B01001006\n",
    "allvariablenames.pop(0) #remove B01001007\n",
    "#--------------------------------------------#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------#\n",
    "#Get the columns ready in Pandas dataframe   #\n",
    "#--------------------------------------------#\n",
    "\n",
    "# load predictors into data frame\n",
    "predictors = alldata[allvariablenames]\n",
    "\n",
    "# load target into data frame\n",
    "target = alldata['# Purchases']\n",
    "\n",
    "\n",
    "#--------------------------------------------#\n",
    "#Split data into train and test sets         #\n",
    "# 30% should be retained for test set        #\n",
    "#--------------------------------------------#\n",
    "\n",
    "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123)\n",
    "\n",
    "#1 pred_train = the predictors training set\n",
    "#2 pred_test = the predictors test set\n",
    "#3 tar_train = the targeting training ste\n",
    "#4 tar_test = the target test set\n",
    "\n",
    "#--------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.496e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.098e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.329e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.051e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=5.739e-01, previous alpha=5.739e-01, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 78 iterations, i.e. alpha=9.605e-02, with an active set of 60 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=5.356e-02, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.802e-02, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.799e-02, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.799e-02, with an active set of 84 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 112 iterations, i.e. alpha=4.757e-02, with an active set of 84 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 113 iterations, alpha=4.799e-02, previous alpha=4.540e-02, with an active set of 84 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.232e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.616e-01, with an active set of 43 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.295e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 62 iterations, alpha=1.281e-01, previous alpha=1.277e-01, with an active set of 53 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.439e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.037e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.200e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.188e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=7.036e-01, previous alpha=6.809e-01, with an active set of 18 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 34 iterations, i.e. alpha=2.443e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 36 iterations, i.e. alpha=2.364e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=1.277e-01, with an active set of 51 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=1.171e-01, with an active set of 57 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=9.407e-02, with an active set of 65 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=9.165e-02, with an active set of 66 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=8.487e-02, with an active set of 68 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 85 iterations, alpha=8.684e-02, previous alpha=8.487e-02, with an active set of 68 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.788e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.100e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.045e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=6.062e-02, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=6.062e-02, with an active set of 61 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=5.439e-02, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 89 iterations, alpha=5.439e-02, previous alpha=5.439e-02, with an active set of 64 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.442e+00, with an active set of 3 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.333e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.051e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.193e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.572e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.446e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.382e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.747e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.997e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.777e-01, previous alpha=1.768e-01, with an active set of 35 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.554e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.277e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.451e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.089e-01, with an active set of 28 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.880e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 43 iterations, alpha=1.742e-01, previous alpha=1.733e-01, with an active set of 38 regressors.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.732e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.285e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/Users/sarahwofford/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.185e-01, previous alpha=5.185e-01, with an active set of 15 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LassoLarsCV(copy_X=True, cv=10, eps=2.220446049250313e-16, fit_intercept=True,\n",
       "      max_iter=500, max_n_alphas=1000, n_jobs=1, normalize=True,\n",
       "      positive=False, precompute=False, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------------------------------#\n",
    "#  Build the LASSO model                     #\n",
    "#--------------------------------------------#\n",
    "\n",
    "#Use the following perameters for the LASSO Model:\n",
    "#    -cv = 10 (this performs a 10-fold cross validation,\n",
    "#      to make sure our results aren't do to random ordering of the data)\n",
    "#    -precompute=False (not necessary)\n",
    "\n",
    "model = LassoLarsCV(cv=10, precompute=False)\n",
    "model.fit(pred_train, tar_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B01001014' 0.8557908775529921]\n",
      "['B01001036' 2.505392496591849]\n",
      "['B01001037' 0.8894214357013622]\n",
      "['B01001038' 1.5315839680821497]\n",
      "['B02001005' 0.4125408937426837]\n",
      "['B13014026' 0.4800240326923769]\n",
      "['B13014027' 0.6977454940063235]\n",
      "['B13016001' 874922971.7249781]\n",
      "['B19001017' 1.4834465563617387]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------#\n",
    "# Build the coefficient chart for examination#\n",
    "#--------------------------------------------#\n",
    "\n",
    "predictors_model=pd.DataFrame(allvariablenames) #converting the list of all variables into a new dataframe to analyze for regression\n",
    "predictors_model.columns = ['label'] #defines the column label for the list of variables in the data frame\n",
    "predictors_model['coeff'] = model.coef_ #pulls the coefficients for each variable from the lasso \"model\" into a new column named \"coeff\"\n",
    "\n",
    "for index, row in predictors_model.iterrows(): #for each index position and row in predictors.model.iterrows() (.iterrows returns a series for each row)\n",
    "    if row ['coeff'] > 0: #if the coefficient in the row being iterated through is greater than 0\n",
    "        print(row.values) #print the value of the coefficient for the variable in that row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTION: If I had to report only two census variables to my boss that most steeply predicted sales,\n",
    "#              what would those be?\n",
    "#       I would report B13016001 (Women aged 15 to 50 years \n",
    "#       (who had birth in the last 12 months by age)) and B01001036 (Females, 30-44 years old).\n",
    "#       Both had the highest coefficients from the census data \n",
    "#       and the LASSO model (874922971.724 and 2.5053924 respectively).  \n",
    "#       I would emphasize that based on the outcome of prediction, that it would be wise \n",
    "#       to target variable B13016001 the most (based on the data) since the coefficient was \n",
    "#        significantly higher than any other variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data MSE\n",
      "22025.312777378716\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[   1    3    4    0  685    4    0    6   40    9    1   45    4   15\n    8    4    4   10    3  556    3    3  133    2    3   16    4   15\n    2   12    2   16    0    2    0    1   14    6    0    3   16    3\n    4    0  700   76    1    4   15    3    1  233    8    1    2    2\n    0    1    1    1   22   10    4    2    1   19    2   84  359  742\n    0    0  684  206   36   12    3  111   37    4    0   14    2   12\n  177    1   32    7    0    0    2   14    3    4    0   19   86    4\n    0    3    6    3    7    2    1    0    2    0 2861   83    0    0\n    2    1    2    0    7    2   20    6   64    2   37    0    3    1\n    1   13   11    1   39    7   10    9    4    0    3    6    4   12\n  217   16    0    0    5    5    3    0    9   14  121   10    0  107\n    0  779  115    2    0    1   80   24  127    4   14    2    0   99\n   10   14    0    1    2   23  328    1    5    6    4   10    2    1\n    1   95   19    9    0    0    2   43   32    6    0    4    0    1\n    9  136    0    0   15   45   12    0   69   16    0    0    0    2\n    0   51   11    0    7   15   72   21  145    6].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-344de8be39e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# run the above code,then do the same thing for test sets (2) and (4) above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpredtest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtar_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Predict Test MSE'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredtest_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \"\"\"\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coef_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[1;32m    241\u001b[0m                                dense_output=True) + self.intercept_\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    439\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0;31m# To ensure that array flags are maintained\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[   1    3    4    0  685    4    0    6   40    9    1   45    4   15\n    8    4    4   10    3  556    3    3  133    2    3   16    4   15\n    2   12    2   16    0    2    0    1   14    6    0    3   16    3\n    4    0  700   76    1    4   15    3    1  233    8    1    2    2\n    0    1    1    1   22   10    4    2    1   19    2   84  359  742\n    0    0  684  206   36   12    3  111   37    4    0   14    2   12\n  177    1   32    7    0    0    2   14    3    4    0   19   86    4\n    0    3    6    3    7    2    1    0    2    0 2861   83    0    0\n    2    1    2    0    7    2   20    6   64    2   37    0    3    1\n    1   13   11    1   39    7   10    9    4    0    3    6    4   12\n  217   16    0    0    5    5    3    0    9   14  121   10    0  107\n    0  779  115    2    0    1   80   24  127    4   14    2    0   99\n   10   14    0    1    2   23  328    1    5    6    4   10    2    1\n    1   95   19    9    0    0    2   43   32    6    0    4    0    1\n    9  136    0    0   15   45   12    0   69   16    0    0    0    2\n    0   51   11    0    7   15   72   21  145    6].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------#\n",
    "# calculate the mean squared error for the training and test set\n",
    "#----------------------------------------------------------------#\n",
    "def mean_squared_error(targets,predictions):\n",
    "    return mean((targets-predictions)**2)\n",
    "\n",
    "train_error = mean_squared_error(tar_train, model.predict(pred_train))\n",
    "print ('training data MSE')\n",
    "print(train_error)\n",
    "\n",
    "# run the above code,then do the same thing for test sets (2) and (4) above.\n",
    "predtest_error = mean_squared_error(pred_test, model.predict(tar_test))\n",
    "print ('Predict Test MSE')\n",
    "print(predtest_error)\n",
    "\n",
    "targtest_error = mean_squared_error(tar_test, model.predict(pred_test))\n",
    "print ('Target Test MSE')\n",
    "print(targtest_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data R-Square\n",
      "0.24002827375880997\n",
      "test data R-Square\n",
      "0.17587122769388464\n",
      "y intercept: \n",
      "22.194697684317433\n"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Based on R Squared values, does the census data predict sales?  #\n",
    "#----------------------------------------------------------------#\n",
    "#No, there is simply not enough data to make such a decision.\n",
    "\n",
    "# calculate the r-squared for the training set:\n",
    "\n",
    "rsquared_train=model.score(pred_train,tar_train)\n",
    "print ('training data R-Square')\n",
    "print(rsquared_train)\n",
    "\n",
    "#repeat for the test set.\n",
    "rsquared_test=model.score(pred_test, tar_test)\n",
    "print ('test data R-Square')\n",
    "print(rsquared_test)\n",
    "\n",
    "# Compare the two R-squareds.\n",
    "\n",
    "#calculate the y-intercept to help interpret baseline sales number\n",
    "print(\"y intercept: \")\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#What is our baseline sales number? \n",
    "#What does that mean, practically?\n",
    "#----------------------------------------------------------------#\n",
    "#Based on the output of the random sample generated above from my model, \n",
    "#the baseline sales number is 22 (products / units).  \n",
    "#Optimally, 22 items are the baseline to predict sales for the product.  \n",
    "#There is then opportunity to increase the amount of products sold if \n",
    "#sold to the correct demographic (census variable). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
